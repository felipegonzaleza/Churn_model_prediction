# -*- coding: utf-8 -*-
"""Tarea 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jH2nBYvGuUmaatqCdZ8XRZjLW85fQf2_

# Tarea II Data Analitycs

---


**Alumnos:
Osvaldo Ceballos /
Felipe Gonzalez /
Juan Ignacio Paredes**
"""

#!pip install -U pandas-profiling

"""# **1. Obtencion de Datos**

---
Cargamos los datos en Google Drive.
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

!ls "/gdrive/My Drive"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import pandas_profiling
from google.colab import files

#Seteamos el seed de manera fija
import random
random.seed(3)

#Graficas
import matplotlib as mpl
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

#from keras.models import Sequential
#from keras.layers.core import Dense
#from keras.optimizers import SGD
#from keras.regularizers import l2

# Ayudas para KDD
import sklearn.model_selection
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn import preprocessing as ppr 

# Modelos
from sklearn import svm
from sklearn import tree
from sklearn.linear_model import LogisticRegression as logit
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn import neighbors as knn 
from sklearn.neural_network import MLPClassifier as mlp
import xgboost as xgb

#Traemos los dos archivos con los que trabajaremos, la base de entrenamiento y la base de testeo. 
df_training = pd.read_csv('/gdrive/My Drive/MIT/BASEFUGA_GENERAL.csv',sep=',')
df_validacion = pd.read_csv('/gdrive/My Drive/MIT/BASEFUGA_VALIDACION.csv',sep=',')

#Seteamos el ID de la base de entrenamiento como indice para separar las variables dependientes de la indepentiente.
df_training.set_index('ID', inplace = True)
df_validacion.set_index('ID', inplace = True)

df_training.info()
df_training.head()
#La base de entrenamiento cuenta con 19 columnas y 2294 registros.

"""# **2. Analisis Exploratorio**

### 2.0 Pandas Profiling
"""

#pandas_profiling.ProfileReport(df_training)
#Con esto hacemos el output a un archivo html en el wd
pandas_profiling.ProfileReport(df_training)
#pandas_profiling.report.to_file(outputfile="output.html")

"""## 2.1 Variable Objetivo

---
Analisamos si nuestra variable objetivo 'FUGA', esta lo suficientemente balanceada, esto es importante, ya que queremos que el modelo sea capaz de identificar ambas clases y no puramente la predominante.
"""

sns.countplot(x=df_training['FUGA'],palette='hls')
plt.show()

"""## 2.2 Variables Numericas

---
A continuacion haremos un analisis de estadistica descriptiva de cada variable numerica.
"""

#EADA
df_training.EDAD.hist()
df_training.EDAD.count()

#EDAD
#e grouped boxplot
sns.boxplot(y='FUGA', x='EDAD', 
                 data=df_training, 
                 palette="colorblind")

#MONTO
df_training.MONTO.hist()

#MONTO
sns.boxplot(x='FUGA', y='MONTO', data = df_training)
#Podemos ver que los clientes con FUGA en promedio tienen ofertas de montos pre aprobados menores que los clientes no fugados.

#RENTA
df_training.RENTA.hist()

"""Para RENTA podemos ver que hay una cola muy pronunciada hacia la derecha, esto lo podriamos corregir aplicando una transformacion Logaritmo sobre esta variable con el objetivo de estirar la distribucion."""

#RENTA
sns.boxplot(x=df_training['FUGA'], y=np.log(df_training['RENTA']))

"""Para ver el Boxplot de Renta sobre Fuga aplicamos Logaritmo a Renta, para tener una escala mejor y que los boxplots se puedieran leer. En un principio no vemos mayores diferencia entre ambas clases de FUGA sobre la renta promedio."""

#M_MOROSO
ax = sns.countplot(x=df_training.M_MOROSO, hue=df_training.FUGA, data=df_training)
plt.show()

#DEUDAS MENSUALES

#DEUDAS MENSUALES

df_deudas = df_training[['D_Marzo', 'D_Abril', 'D_Mayo', 'D_Junio', 'D_Julio', 'D_Agosto', 'D_Septiembre', 'FUGA']].copy()
df_deudas.groupby('FUGA').mean()
df_deudas.hist()

#DEUDAS MENSUALES para "clientes FUGA"
ax = sns.boxplot(data=df_deudas[df_deudas.FUGA =='FUGA'], orient='h', palette='Set2')

#DEUDAS MENSUALES para "clientes NO FUGA"
ax = sns.boxplot(data=df_deudas[df_deudas.FUGA =='NO FUGA'], orient='h', palette='Set2')

"""Revisando los histogramas, medias y boxplots de las variables de deuda, en primer lugar, nos damos cuenta que hay variables en escalas diferentes. En segundo lugar, no vemos mayores diferencias entre ambas clases de FUGA sobre las deudas.

## 2.3 Variables Categoricas
"""

#GENERO
ax = sns.countplot(x=df_training.GENERO, hue=df_training.FUGA, data=df_training)
plt.show()

#NIV_EDUC
#Podemos ver en el grafico a continuacion que hay diferentes proporciones entre clientes fugados para cada tipo de Nivel Educacional, por lo que podria ser una buena variable. La mantenemos.
ax = sns.countplot(x=df_training.NIV_EDUC, hue=df_training.FUGA, data=df_training)
plt.show()

#E_CIVIL
ax = sns.countplot(x=df_training.E_CIVIL, hue=df_training.FUGA, data=df_training)
plt.show()

#SEGURO
ax = sns.countplot(x=df_training.SEGURO, hue=df_training.FUGA, data=df_training)
plt.show()

"""## 2.4 Valores Nulos"""

#Revisamos cuantos valores nulos tenemos.
df_training.isnull().sum()

#Aqui eliminamos los registros que tengan valores nulos para las variables GENERO, NIV_EDUC y E_CIVIL. 
df_training = df_training.dropna(subset=['GENERO', 'NIV_EDUC','E_CIVIL'])
df_validacion= df_validacion.dropna(subset=['GENERO', 'NIV_EDUC','E_CIVIL'])

df_training.isnull().sum()
df_validacion.isnull().sum()

df_training.shape
#En total, savando los valores nulos, eliminamos 33 registros.

"""# **3. Transformación de variables**

---
A continuacion procederemos a crear el dataframe final para el entrenamiento del modelo.

## 3.1 Eliminacion Variables

---
Dados los resultados del Pandas Profiling tomamos la decision de eliminar las variables de COD_OFI y COD_COM, ya que su dispercion es muy alta para ser variables categoricas.
"""

#Eliminacion de variables. 
del df_training['COD_OFI']
del df_training['COD_COM']

"""## 3.2 Filtro Outliers

---
En esta etapa limpiaremos la variable EDAD, ya que como vimos en el EDA, tiene incluso valores negativos, lo cual es ilogico.
"""

#EDAD
print(df_training.EDAD.value_counts)
df_training = df_training.query('EDAD < 101 & EDAD > 18')
print(df_training.EDAD.value_counts)

"""**Con el filtro de edad aplicado [18,100], perdimos 7 registros.**"""

#Con la extraccion de estos datos de la base, quedamos con 2286 registros validos, en un comienzo teniamos 2294, por lo que perdimos 8 registros.
sns.boxplot(x=df_training["FUGA"],y=df_training['EDAD'])
#Se observa que la variable edad es una buena variable predictiva de la fuga.

#EDAD en Validacion. 
df_validacion = df_validacion.query('EDAD < 101 & EDAD > 18')

"""## 3.2 Encoding

---
A continuacion aplicaremos varios tipos de encoding para variables categoricas.
"""

#CIUDAD

df_training['CIUDAD'] = np.where(df_training['CIUDAD'].str.contains('SANTIAGO'), 1, 0)
df_validacion['CIUDAD'] = np.where(df_validacion['CIUDAD'].str.contains('SANTIAGO'),1,0)

#CIUDAD 
#Ahora revisamos la proporcion que queda entre Saniago y el resto de las ciudades. 
sns.countplot(x=df_training['CIUDAD'],palette='hls')
plt.show()
pd.crosstab(df_training['CIUDAD'], 
                          df_training['FUGA'],  
                            margins = False)

#E_CIVIL
#GENERO
#SEGURO
#FUGA

#ENDOCING 1 0

df_training['E_CIVIL'] = np.where(df_training['E_CIVIL'].str.contains('CAS'), 1, 0)
df_training['GENERO'] = np.where(df_training['GENERO'].str.contains('M'), 1, 0)
df_training['SEGURO'] = np.where(df_training['SEGURO'].str.contains('SI'), 1, 0)
df_training['FUGA'] = np.where(df_training['FUGA'].str.contains('NO FUGA'), 0, 1)

#ENCODING 1 0 para Validacion

df_validacion['E_CIVIL'] = np.where(df_validacion['E_CIVIL'].str.contains('CAS'), 1, 0)
df_validacion['GENERO'] = np.where(df_validacion['GENERO'].str.contains('M'), 1, 0)
df_validacion['SEGURO'] = np.where(df_validacion['SEGURO'].str.contains('SI'), 1, 0)

#NIV_EDUC
#Para la variable NIV_EDUC vemos que existen 3 clases predominantes. 
sns.countplot(x=df_training['NIV_EDUC'],palette='hls')
plt.show()

#ENCODING ORDINAL - Para la variable NIV_EDUC crearemos un encoding ordinal de 1 a 5.
#Asumiremos que EUN es un dato mal imputado y que corresponde a UNV, siendo ambos educacion Universitaria. 

df_training['NIV_EDUC'] = df_training['NIV_EDUC'].replace(['BAS', 'MED', 'TEC', 'EUN', 'UNV'],[1, 2, 3, 4, 4])
df_validacion['NIV_EDUC'] = df_validacion['NIV_EDUC'].replace(['BAS', 'MED', 'TEC', 'EUN', 'UNV'],[1, 2, 3, 4, 4])

#Vemos si el nivel educacional dice algo de la fuga. 

sns.boxplot(x='FUGA', y='NIV_EDUC', data = df_training)

#Donde se ve que la variable modificada es una excelente variable predictiva

"""**Como resultado del Encoding Ordinal que realizamos para NIV_EDUC podemos concluir que los clientes fugados tienen en promedio nivel de educacion mas alto que los no fugados.**

## 3.3 Creacion Variables
"""

#Crearemos una nueva Variable llamada LEVERAGE, que sera la diferencia entre la deuda al comienzo del periodo y al final. 
#Para cada periodo usaremos 3 meses, dejando fuera a la variable D_Marzo. 

df_training['LEVERAGE1'] = (df_training['D_Julio'] + df_training['D_Agosto'] + df_training['D_Septiembre']) 
df_training['LEVERAGE2'] = (df_training['D_Abril'] + df_training['D_Mayo'] + df_training['D_Junio'])

#No posemoa aplicar logaritmo a la diferencia, ya que no existe el logaritmo de un valor negativo. 
df_training['DIF_LEVERAGE'] = np.sign(df_training['LEVERAGE1'] - df_training['LEVERAGE2']) * np.log(np.abs(df_training['LEVERAGE1'] - df_training['LEVERAGE2'])/df_training['RENTA'] +1)
sns.boxplot(x=df_training['FUGA'], y=df_training['DIF_LEVERAGE'])
#df_training[df_training['DIF_LEVERAGE'].isnull()] pa sapear
df_training['DIF_LEVERAGE'].fillna(0, inplace=True)
df_training[df_training['DIF_LEVERAGE'].isnull()]

#Crearemos una nueva Variable llamada LEVERAGE, 

df_validacion['LEVERAGE1'] = (df_validacion['D_Julio'] + df_validacion['D_Agosto'] + df_validacion['D_Septiembre']) 
df_validacion['LEVERAGE2'] = (df_validacion['D_Abril'] + df_validacion['D_Mayo'] + df_validacion['D_Junio'])

df_validacion['DIF_LEVERAGE'] = np.sign(df_validacion['LEVERAGE1'] - df_validacion['LEVERAGE2']) * np.log(np.abs(df_validacion['LEVERAGE1'] - df_validacion['LEVERAGE2'])/(df_validacion['RENTA'])+1)


#df_training[df_training['DIF_LEVERAGE'].isnull()] pa sapear
df_validacion['DIF_LEVERAGE'].fillna(0, inplace=True)
df_validacion[df_validacion['DIF_LEVERAGE'].isnull()]

#Deuda_Promedio, sera la suma de la deuda del periodo divido 7.
df_training['Deuda_Promedio'] = (df_training.D_Marzo + df_training.D_Abril + df_training.D_Mayo + df_training.D_Junio + df_training.D_Julio + df_training.D_Agosto + df_training.D_Septiembre) / 7
df_validacion['Deuda_Promedio'] = (df_validacion.D_Marzo + df_validacion.D_Abril + df_validacion.D_Mayo + df_validacion.D_Junio + df_validacion.D_Julio + df_validacion.D_Agosto + df_validacion.D_Septiembre) / 7

#Escalamiento LOG de Deuda_Promedio
df_training['Deuda_Promedio'] = np.log(df_training['Deuda_Promedio'] +1)
df_validacion['Deuda_Promedio'] = np.log(df_validacion['Deuda_Promedio']+1)

df_training.describe()

df_training.head()

"""## 3.4 Transformacion Log"""

#RENTA
df_training.RENTA.hist()

#RENTA
#Crearemos una nueva variables llamada Renta_Log, que es el logaritmo de RENTA.
df_training['RENTA'] = np.log(df_training['RENTA'] +1 )
df_validacion['RENTA'] = np.log(df_validacion['RENTA'] +1)
df_training.RENTA.hist()

sns.boxplot(x='FUGA', y=np.log(df_training['RENTA']+1), data = df_training)

"""## 3.6 Escalamiento"""

#EDAD
#MONTO

#Ahora escalaremos las variables numericas con el mismo escaler que usamos para la base de entrenamiento. 
#z = (x - u) / s

# Promedios (u):
u_edad = df_training.EDAD.mean()
u_monto = df_training.MONTO.mean()

#Desviaciones Standar (s): 
stdv_edad = df_training.EDAD.std()
stdv_monto = df_training.MONTO.std()

#Agremos 2 columnas nuevas: 
df_training['EDAD_2'] = df_training['EDAD']
df_training['MONTO_2'] = df_training['MONTO']

#Hacemos la transformacion:
df_training['EDAD_2'] = (df_training['EDAD_2'] - u_edad) / stdv_edad
df_training['MONTO_2'] = (df_training['MONTO_2'] - u_monto) / stdv_monto

"""## 3.7 Dataframe Final"""

df_training.head()

df_validacion.head()

"""# **4. Modelamiento**

## 4.1 Separamos X e Y.
"""

#Separamos la variables Y de las varuables independientes X, y las transformamos de array a dataframe:

#ENTRENAMIENTO
Y = pd.DataFrame(data=df_training,columns=['FUGA'])
X = pd.DataFrame(data=df_training,columns= ['GENERO','RENTA','EDAD','NIV_EDUC','E_CIVIL','CIUDAD','M_MOROSO','MONTO','SEGURO','DIF_LEVERAGE'])
X.head()

#VALIDACION
Y_val = pd.DataFrame(data=df_validacion,columns=['FUGA'])
X_val = pd.DataFrame(data=df_validacion,columns= ['GENERO','RENTA','EDAD','NIV_EDUC','E_CIVIL','CIUDAD','M_MOROSO','MONTO','SEGURO','DIF_LEVERAGE'])

X.head(10)

X_val.head(10)

"""## 4.2 Data Mining
----
Se realizara una comparación del desempeño de distintos modelos de clasificación utilizando 10-fold Cross Validation.
Se revisarán las metricas de accuracy, sensitivity (recall) y AUC. Con ello se definirá un modelo de buen desempeño, el que pasaremos a optiimizar sus hiperparámetros a traves de un grid search del modelo campeón.
"""

#Escalar los datos y verificar nulos
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns= X.columns)
type(X)
X.isnull().any()

X_val = pd.DataFrame(scaler.transform(X_val), columns = X_val.columns)
type(X_val)
X_val.isnull().any()

X.head(10)

X_val.head(10)

"""## 4.3 Comparacion de Modelos"""

#Comparación de modelos

models = []

models.append(("LogisticRegression",sklearn.linear_model.LogisticRegression()))
models.append(("SVC",sklearn.svm.SVC()))
models.append(("RandomForest",sklearn.ensemble.RandomForestClassifier()))
models.append(("KNeighbors",sklearn.neighbors.KNeighborsClassifier()))
models.append(("MLPClassifier",sklearn.neural_network.MLPClassifier()))
models.append(("XGBoost",xgb.XGBClassifier()))  
models.append(("DecisionTree",sklearn.tree.DecisionTreeClassifier()))

# Segun Accuracy
results = []
names = []
for name,model in models:
    result = sklearn.model_selection.cross_val_score(model, 
                             X, 
                             Y,  
                             cv=10, 
                             scoring='accuracy',
                             n_jobs=-1
                            )
    names.append(name)
    results.append(result)

models

plt.boxplot(results,labels = names)
plt.xticks(rotation=45)
plt.show()

#Segun Sensitivity
results = []
names = []
for name,model in models:
    result = sklearn.model_selection.cross_val_score(model, 
                             X, 
                             Y,  
                             cv=10, 
                             scoring='recall',
                             n_jobs=-1
                            )
    names.append(name)
    results.append(result)

plt.boxplot(results,labels = names)
plt.xticks(rotation=45)
plt.show()

#Segun AUC
results = []
names = []
for name,model in models:
    result = sklearn.model_selection.cross_val_score(model, 
                             X, 
                             Y,  
                             cv=10, 
                             scoring='roc_auc',
                             n_jobs=-1
                            )
    names.append(name)
    results.append(result)

plt.boxplot(results,labels = names)
plt.xticks(rotation=45)
plt.show()

"""Nos quedamos con Extreme Gradient Boosting por tener un buen desempeño tanto en accuracy 87.5% , como por detectar bien la clase fuga con un recall de 89% y con un AUC de 0.95 bajo la curva ROC."""

x_train, x_test,y_train, y_test = sklearn.model_selection.train_test_split(
    X,
    Y,
    test_size=0.30,
    random_state = 3,
    stratify = Y #Solo para generalizar el problema de fuga ya que la base es balanceada
)

fuga_dmatrix = xgb.DMatrix(data=X, label=Y)

"""## 4.3 Grid Search"""

# Grid Search (optimización de hiperparámetros) para XGBoost
parameters = {#'base_score' : [0.3, 0.5, 0.7], 
              'learning_rate' : [0.01,0.05,0.1], # Velocidad o salto de reducción de error
              'max_depth' : [4,5,6] , # Maxima profundidad del arbol
              'colsample_by_tree': [0.3, 0.5, 0.7], # % Features usados por arbol
              'subsample': [1], # % ejemplos usados por arbol
              'n_estimators': [50,100,200],
              'min_child_weight': [1, 5, 10],
              'gamma': [0, 0.5, 1], #Regularizacion opcion 1
              #'alpha' : [0, 0.5, 1], # Regularización L1
              #'lambda' : [0, 0.5, 1] # Regularización L2
              'seed' : [3] 
}

grid_Search = sklearn.model_selection.GridSearchCV(xgb.XGBClassifier(), 
                                                    parameters, 
                                                    n_jobs=-1,
                                                    cv = 3,
                                                    scoring = 'roc_auc',
                                                   )

grid_Search.fit(x_train,y_train)

grid_Search.best_params_

"""## 4.4 Feature Important List"""

#View Model
# fit model no training data
model = xgb.XGBClassifier(base_score = 0.3,
                      gamma= 0,
                      learning_rate = 0.1,
                      max_depth = 6,
                      min_child_weight = 1,
                      n_estimators = 50,
                      seed =3)
model.fit(x_train, y_train)

#xgb.plot_tree(model, num_tree = 1, rankdir = "LR")

# feature importance
# plot feature importance
xgb.plot_importance(model, importance_type = 'gain')
plt.show()

pd.DataFrame(grid_Search.cv_results_).head(10)

resultado = grid_Search.predict(x_test) #Prediccion base con punto de corte al 50%
proba = grid_Search.predict_proba(x_test) #Probabilidad de fuga

print(classification_report(resultado, y_test))

print(proba)
type(proba)

"""Ahora con este modelo entrenado veremos la predicción en la base de validación, en la que no tiene label, para poder realizar el modelo de decisión en base al costo/utilidad."""

Y_val["FUGA"] = grid_Search.predict(X_val)
proba_val = grid_Search.predict_proba(X_val)

#Revisaremos si la variable independiente esta balanceada. 
sns.countplot(x=Y_val["FUGA"],palette='hls')
plt.show()
Y_val['FUGA'].value_counts()

Y['FUGA'].value_counts()

1116/(1116+1138)

574/(574+604)

"""Lo de abajo no aplica porque las proporciones de fugados tanto en la base fuga como en la de validacion son iguales.

Para poder realizar la selección correcta, debemos volver a estratificar la base de test, evaluamos 3 alternativas para poder ralizar esta tarea.

1.- Desbalancear la muestra original de entrenamiento en un porcentaje que mantanga la proporción estimada de la base de validación. El problema de esta técnica es que los modelos prefieren una base balanceada de entrenamiento, así que no tendría mucho sentido empobrecer el entrenamiento dandole una base desbalanceada si ya se cuenta con una balanceada.

2.- Subsampling de la base de testeo. Habíamos realizado un train test split de un 70/30, por lo que la cantidad de casos en el conjunto de test es de un X. Como esos casos se encuentran en la misma proporcón que la base general, se pueden sacar casos fugados para llegar a la proporción de validación. El problema es que la base es muy pequeña por lo que podríamos perder escenarios importantes de la validación de los fugados.

3.- Oversampling rescatando casos de la base de entrenamiento: Esto es , mantener los casos de fuga de la base de test, pero además añadir casos no-fugados desde la base de training. Problema information leak

4. Añadir muestras sinteticas de casos de no-fugados. Utilizando SMOTE
"""

#from imblearn.over_sampling import SMOTE
# RandomOverSampler
  # With over-sampling methods, the number of samples in a class
  # should be greater or equal to the original number of samples.
# SMOTE
#sampler = SMOTE(ratio={1: 1927, 0: 300},sampling_strategy= 'not minority' ,random_state=3)
#X_rs, y_rs = sampler.fit_sample(x_test, y)
#print('SMOTE {}'.format(Counter(y_rs)))
#plot_this(X_rs,y_rs)

"""# **5. Modelo de decisión**



---
Con el modelo anterior tenemos un probabilidad de fuga bastante creible. En esta estapa trataremos de encontrar el punto de corte optimo para la P(x) ~ Y' tal que si se tiene una función de utilidad U: x,P(x) -> $, entonces MAX(U(x, y_hat)). Notar que para esta tarea se está asumiendo que tenemos una capacidad de retención del 100% de los clientes, cosa que es una simplicficación de la realidad.

En particular se sabe que la matriz de costo/beneficio tiene la siguiente estructura.
"""

mat_costo_ben = np.array([[0, -5000],[-100,1000]]) #En el orden entregado por sklearn
print(mat_costo_ben)
type(mat_costo_ben)

mat_confusion =  sklearn.metrics.confusion_matrix(resultado, y_test)
print(mat_confusion)
type(mat_confusion)
Beneficio = np.sum(np.multiply(mat_costo_ben, mat_confusion)) #esto hay que concvertirlo en función con def wea 
print(Beneficio)
salida = pd.DataFrame(proba)

salida.head(10)
#salida[1].head(10)

salida = pd.DataFrame(proba)

def Utilidad(corte,probabilidad,y_test):
    mat_costo_ben = np.array([[0, -5000],[-100,1000]]) 
    y_prima = probabilidad.apply(lambda x: 1 if x >= corte else 0)
    mat_confusion =  sklearn.metrics.confusion_matrix(y_prima, y_test)
    return np.sum(np.multiply(mat_costo_ben,mat_confusion)) 

def Perdida(corte,*args):
    probabilidad, y_test = args[0], args[1]
    mat_costo_ben = np.array([[0, 5000],[100,-1000]]) 
    y_prima = probabilidad.apply(lambda x: 1 if x >= corte else 0)
    mat_confusion =  sklearn.metrics.confusion_matrix(y_prima, y_test)
    return np.sum(np.multiply(mat_costo_ben,mat_confusion))

print(Utilidad(0.91133239,salida[1],y_test))

from scipy import optimize

argumentos = {'probabilidad': salida[1], 
              'y_test': y_test}
argumentos_2 = (salida[1], y_test)

punto_corte= optimize.fmin(func = Perdida,x0 =0.5 ,args=argumentos_2 )
print(punto_corte)

seq = np.linspace(0,1,1001)
U_fun = np.zeros(1001)
print(seq)

seq = np.linspace(0,1000,1001, dtype = 'int16')
U_fun = np.zeros(1001)
for i in np.nditer(seq):
  U_fun[i] = Utilidad(i/1000,argumentos_2[0], y_test =argumentos_2[1])

plt.plot(seq/1000, U_fun)

"""# **7. Base Resultado**"""

df_salida = pd.read_csv('/gdrive/My Drive/MIT/BASEFUGA_VALIDACION.csv',sep=',')

#Seteamos el ID de la base de entrenamiento como indice para separar las variables dependientes de la indepentiente.
df_salida.set_index('ID', inplace = True)
df_salida['FUGA'] = Y_val['FUGA']

indices = Y_val.index.values
df_salida.loc[indices,'P_FUGA'] = proba_val[:,1]



df_salida['FUGA_PRIMA'] = np.where(df_salida.P_FUGA > punto_corte[0], 1, 0)
df_salida.head(5)

df_salida.to_csv('/gdrive/My Drive/MIT/BASEFUGA_VALIDACION_SALIDA.csv')

"""# **6. Clustering (KMeans)**"""

from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min


df_kmeans = df_training[['RENTA','EDAD_2','MONTO_2']]
df_kmeans

#Kmeans
X_k = np.array(df_training[['RENTA','EDAD_2','MONTO_2']])
y_k = np.array(df_training['FUGA'])
X_k.shape

Nc = range(1, 20)
kmeans = [KMeans(n_clusters=i) for i in Nc]
kmeans
score = [kmeans[i].fit(X_k).score(X_k) for i in range(len(kmeans))]
score
plt.plot(Nc,score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()

#Cluster the data
kmeans_2 = KMeans(n_clusters=5, random_state=0).fit(df_kmeans)
labels_2 = kmeans_2.labels_

df_kmeans['clusters'] = labels_2
df_kmeans.head(20)

C = kmeans_2.cluster_centers_
C

#VISUALIZACION 3D
from mpl_toolkits.mplot3d import Axes3D

colores=['red','green','blue','cyan','yellow']
asignar=[]
for row in labels_2:
    asignar.append(colores[row])

fig = plt.figure()
ax = Axes3D(fig)
ax.scatter(df_kmeans.RENTA, df_kmeans.EDAD_2, df_kmeans.MONTO_2, c=asignar,s=60)
ax.scatter(C[:, 0], C[:, 1], C[:, 2], marker='*', c=colores, s=1000)